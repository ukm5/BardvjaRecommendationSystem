{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating normalized vectors for the physics\n",
      "completed RegEx on title!\n",
      "completed RegEx on abstract!\n",
      "completed WordNinja on title!\n",
      "completed WordNinja on abstract!\n",
      "completed Porter Stemming title!\n",
      "completed Porter Stemming title!\n",
      "completed adding title and abstract!\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (25890, 501)\n",
      "Creating normalized vectors for the fluid\n",
      "completed RegEx on title!\n",
      "completed RegEx on abstract!\n",
      "completed WordNinja on title!\n",
      "completed WordNinja on abstract!\n",
      "completed Porter Stemming title!\n",
      "completed Porter Stemming title!\n",
      "completed adding title and abstract!\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (25987, 501)\n",
      "Creating normalized vectors for the particle\n",
      "completed RegEx on title!\n",
      "completed RegEx on abstract!\n",
      "completed WordNinja on title!\n",
      "completed WordNinja on abstract!\n",
      "completed Porter Stemming title!\n",
      "completed Porter Stemming title!\n",
      "completed adding title and abstract!\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (29951, 501)\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from global_params import search_queries, path_to_arxiv_data, path_to_trained_models\n",
    "\n",
    "def pre_processing_pdf_text(df):\n",
    "    import pandas as pd\n",
    "    import wordninja\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    import regex as re\n",
    "\n",
    "    df['title'] = df['title'].map(lambda x: re.sub('\\d+',\"\",x))\n",
    "    print(\"completed RegEx on title!\")\n",
    "    df['abstract'] = df['abstract'].map(lambda x: re.sub('\\d+',\"\",x))\n",
    "    print(\"completed RegEx on abstract!\")\n",
    "    df['title'] = df['title'].map(lambda x: wordninja.split(x.lower()))\n",
    "    print(\"completed WordNinja on title!\")\n",
    "    df['abstract'] = df['abstract'].map(lambda x: wordninja.split(x.lower()))\n",
    "    print(\"completed WordNinja on abstract!\")\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    df['title'] = df['title'].map(lambda x: ' '.join(list(map(porter.stem, x))))\n",
    "    print(\"completed Porter Stemming title!\")\n",
    "    df['abstract'] = df['abstract'].map(lambda x: ' '.join(list(map(porter.stem, x))))\n",
    "    print(\"completed Porter Stemming abstract!\")\n",
    "    \n",
    "    df['complete_pdf'] = df['abstract']*2 + ' ' + (df['title']+' ')*10\n",
    "    print(\"completed adding title and abstract!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# abstract here refer to the complete_pdf column we have made\n",
    "def transform_abstracts_to_vectors(df):\n",
    "    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "    from sklearn.preprocessing import normalize\n",
    "    from scipy import sparse\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    import string\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    if not (os.path.isdir(f'{path_to_trained_models}')):\n",
    "        print(\"Missing directory for trained models\")\n",
    "        print(f\"Creating a new directory path: {path_to_trained_models}\")\n",
    "        os.mkdir(f'{path_to_trained_models}')\n",
    "    \n",
    "    X = df['complete_pdf']\n",
    "    porter = PorterStemmer()\n",
    "    stop_words_english = joblib.load(f'{path_to_trained_models}stop_words_english')\n",
    "\n",
    "    chars = [c for c in string.ascii_lowercase]\n",
    "    stop_words_english += chars\n",
    "    for i in chars:\n",
    "        for j in chars:\n",
    "            stop_words_english.append(i+j)\n",
    "            \n",
    "    # Load the vectorizer models\n",
    "    cvec = joblib.load(f'{path_to_trained_models}count_vectorizer.sav')\n",
    "    tvec = joblib.load(f'{path_to_trained_models}tfidf_vectorizer.sav')\n",
    "    X_vec = cvec.transform(X)\n",
    "    X_vec2 = tvec.transform(X)   \n",
    "\n",
    "    # Load the Standard Scaler model\n",
    "    ss = joblib.load(f'{path_to_trained_models}standard_scaler.sav')\n",
    "    X_vec_ss = ss.transform(X_vec.toarray())\n",
    "\n",
    "    # Load the KMeans cluster model\n",
    "    k_cluster = joblib.load(f'{path_to_trained_models}kmeans_cluster.sav')    \n",
    "\n",
    "    df['cluster_label'] = k_cluster.predict(X_vec_ss)\n",
    "    X_vec = np.append(X_vec.toarray(),df['cluster_label'].values.reshape(-1,1), axis=1)\n",
    "    X_normal = normalize(sparse.csr_matrix(X_vec)).toarray()\n",
    "    \n",
    "    return X_normal\n",
    "\n",
    "for search_query in search_queries:\n",
    "    print(f\"Creating normalized vectors for the {search_query}\")\n",
    "    df_arxiv = pd.read_csv(f'{path_to_arxiv_data}arxiv_{search_query}_30000.csv')\n",
    "    pre_processing_pdf_text(df_arxiv)\n",
    "    df_arxiv.to_csv(f'{path_to_arxiv_data}arxiv_{search_query}_30000_preprocessed.csv', index=False)\n",
    "    df_arxiv = pd.read_csv(f'{path_to_arxiv_data}arxiv_{search_query}_30000_preprocessed.csv')\n",
    "    Z = transform_abstracts_to_vectors(df_arxiv)\n",
    "    Z_sparse = sparse.csr_matrix(Z)\n",
    "    sparse.save_npz(f'{path_to_arxiv_data}normalized_arxiv_paper_vectors_{search_query}.npz', matrix=Z_sparse)\n",
    "    print(type(Z_sparse), Z_sparse.shape)\n",
    "\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
