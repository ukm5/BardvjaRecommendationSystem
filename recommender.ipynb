{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bardvja is thinking ...\n",
      "Please sit down, have some tea!\n",
      "Thank you for waiting ... \n",
      "\n",
      " The papers in order of relevance are at ./../arxiv_data/\n",
      "\n",
      " Here are ten papers and their links for you\n",
      "                                                title  \\\n",
      "0   Particle accretion onto planets in discs with ...   \n",
      "1   Slipping motion of large neutrally-buoyant par...   \n",
      "2   Restricted Euler dynamics along trajectories o...   \n",
      "3   Interface-resolved simulations of small inerti...   \n",
      "4   Particle dynamics in discs with turbulence gen...   \n",
      "5   Particle-laden two-dimensional elastic turbulence   \n",
      "6   Modulation of the turbulence regeneration cycl...   \n",
      "7   Some issues concerning Large-Eddy Simulation o...   \n",
      "8   Turbulence modulation in heavy-loaded suspensi...   \n",
      "9        Wetting and particle adsorption in nanoflows   \n",
      "10  Simulation of deterministic energy-balance par...   \n",
      "\n",
      "                                         id  \n",
      "0         http://arxiv.org/abs/1803.08730v2  \n",
      "1          http://arxiv.org/abs/1306.1388v1  \n",
      "2         http://arxiv.org/abs/1608.02464v1  \n",
      "3         http://arxiv.org/abs/1906.01249v5  \n",
      "4         http://arxiv.org/abs/1607.02322v1  \n",
      "5         http://arxiv.org/abs/1804.01441v1  \n",
      "6         http://arxiv.org/abs/1807.02107v1  \n",
      "7          http://arxiv.org/abs/0801.2365v1  \n",
      "8         http://arxiv.org/abs/1702.00397v1  \n",
      "9   http://arxiv.org/abs/cond-mat/0406291v1  \n",
      "10        http://arxiv.org/abs/1701.02346v1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from global_params import search_queries, path_to_arxiv_data, path_to_training_data, path_to_trained_models\n",
    "import sys\n",
    "\n",
    "print(\" Bardvja is thinking ...\")\n",
    "print(\" Please sit down, have some tea!\")\n",
    "\n",
    "# closer the function to 1 better match the paper is\n",
    "def cosine_similarity_maximizer(array1, array2):\n",
    "    # array1 is the local vectors\n",
    "    # array2 is the vectors in arxiv\n",
    "    # calculates the cosine with every vector in the arxiv\n",
    "    # returns the minimum of cosine with all vectors in array1\n",
    "    import numpy as np\n",
    "    dot_prod = np.dot(array2, array1.T)\n",
    "    cos_maximizer = []\n",
    "    cos_maximizer = [np.mean(row) for row in dot_prod]\n",
    "    return np.array(cos_maximizer)\n",
    "\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "# Read in the data files for arxiv and the local papers\n",
    "df_local = pd.read_csv(f'{path_to_training_data}local_papers.csv')\n",
    "df_to_concat = [pd.read_csv(f'{path_to_arxiv_data}arxiv_{query}_30000.csv') for query in search_queries]\n",
    "df_arxiv = pd.concat(df_to_concat, join='outer')\n",
    "\n",
    "# Load the normalized vectors\n",
    "local_array = sparse.load_npz(f'{path_to_training_data}normalized_train_vectors.npz')\n",
    "\n",
    "arrays_to_concat = [\n",
    "    sparse.load_npz(f'{path_to_arxiv_data}normalized_arxiv_paper_vectors_{query}.npz').todense() for query in search_queries\n",
    "]\n",
    "arxiv_array = np.concatenate(arrays_to_concat, axis = 0)\n",
    "arxiv_array = sparse.csr_matrix(arxiv_array)\n",
    "sparse.save_npz(f'{path_to_arxiv_data}normalized_arxiv_paper_vectors.npz', matrix=arxiv_array)\n",
    "\n",
    "# Comment these if you are not creating a new recommender\n",
    "cos_maximizer = cosine_similarity_maximizer(local_array, arxiv_array)\n",
    "\n",
    "joblib.dump(cos_maximizer,f'{path_to_arxiv_data}cosine_similarity_maximizer')\n",
    "\n",
    "cos_maximizer = joblib.load(f'{path_to_arxiv_data}cosine_similarity_maximizer')\n",
    "\n",
    "df_arxiv['cosine_similarity_maximizer'] = cos_maximizer\n",
    "\n",
    "df_arxiv_arranged = df_arxiv.sort_values(by='cosine_similarity_maximizer', ascending=False).drop_duplicates()\n",
    "df_arxiv_arranged.reset_index(drop=True, inplace=True)\n",
    "df_arxiv_arranged.to_csv(f'{path_to_arxiv_data}df_arxiv_arranged.csv', index=False)\n",
    "\n",
    "vocab = joblib.load(f'{path_to_trained_models}cvec_vocabulary')\n",
    "\n",
    "print(\"\\n Thank you for waiting ... \")\n",
    "print(f\"\\n The papers in order of relevance are at {path_to_arxiv_data}\")\n",
    "print(\"\\n Here are ten papers and their links for you\")\n",
    "print(df_arxiv_arranged.loc[:10, ['title','id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
