{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search queries obtained to perform arxiv search:  ['physics', 'fluid', 'particle']\n",
      "Starting scraping arXiv for papers under 'physics' ...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from global_params import search_queries, path_to_arxiv_data\n",
    "import sys\n",
    "\n",
    "if not (os.path.isdir(f'{path_to_arxiv_data}')):\n",
    "    print(\"Missing directory for data from arxiv\")\n",
    "    print(f\"Creating a new directory path: {path_to_arxiv_data}\")\n",
    "    os.mkdir(f'{path_to_arxiv_data}')\n",
    "\n",
    "print(\"Search queries obtained to perform arxiv search: \",search_queries)\n",
    "    \n",
    "def one_big_scraping_function(query):\n",
    "    # Dictionary to contain the papers from arxiv\n",
    "    # The dict is converted to a dataframe\n",
    "    df_dict = {\n",
    "        'title':[], \n",
    "        'id':[],\n",
    "        'authors':[],\n",
    "        'abstract':[],\n",
    "        'journal':[],\n",
    "        'published':[]\n",
    "    }\n",
    "\n",
    "    df_arxiv = pd.DataFrame(df_dict)\n",
    "\n",
    "    # Base url = http://export.arxiv.org/api/\n",
    "    # method_name = query\n",
    "    # Here this is actual Base_url + method + ?\n",
    "    base_url = 'http://export.arxiv.org/api/query?'\n",
    "    search_query = query\n",
    "    id_list = ''\n",
    "    start = 0\n",
    "    max_results = 10\n",
    "    sortBy = 'relevance'\n",
    "    sortOrder = 'descending'\n",
    "\n",
    "    # Create the exact url that is parsed to obtain papers\n",
    "    def url_to_scrape(base_url='http://export.arxiv.org/api/query?',search_query = '',id_list = '',start = 0,max_results = 10,sortBy = 'relevance',sortOrder = 'ascending'):\n",
    "        url = base_url+'search_query='+search_query+'&'+'id_list='+id_list+'&'+'start='+str(start)+'&'+'max_results='+str(max_results)+'&'+'sortBy='+sortBy+'&'+'sortOrder='+sortOrder\n",
    "        return url\n",
    "\n",
    "    # Parsing the page with paper info\n",
    "    def scraping_arxiv(total_papers = 10000, max_results=2000, search_query = 'all:physics+OR+mathematics+OR+biology+OR+statistics+OR+politics'):\n",
    "        for i in list(range(0, total_papers, max_results)):\n",
    "            start = i\n",
    "            res = requests.get(url_to_scrape(start = start,max_results = max_results, search_query = search_query))\n",
    "            if (res.status_code == 200):\n",
    "                soup = BeautifulSoup(res.content, 'lxml')\n",
    "                yield soup\n",
    "            else:\n",
    "                print(f'Have an error scraping for {search_query}:', res.status_code)\n",
    "\n",
    "    # Extracting details to be stored in dict\n",
    "    def paper_info(paper_details):\n",
    "        title = paper_details.find('title').text\n",
    "        urlid = paper_details.find('id').text\n",
    "        published = pd.to_datetime(paper_details.find('published').text)\n",
    "        abstract = paper_details.find('summary').text\n",
    "        authors = [authors.find('name').text for authors in paper_details.find_all('author')]\n",
    "        try:\n",
    "            journal = paper_details.find('arxiv:journal_ref').text\n",
    "        except:\n",
    "            journal = 'None'\n",
    "        dict_row = {\n",
    "            'title':title, \n",
    "            'urlid':urlid,\n",
    "            'published':published,\n",
    "            'abstract':abstract,\n",
    "            'authors':authors,\n",
    "            'journal':journal\n",
    "        }\n",
    "        return dict_row\n",
    "\n",
    "    # Adding the info into a dict\n",
    "    def create_dict(obj, df_dict):\n",
    "        count=0\n",
    "        for paper in obj.find_all('entry'):\n",
    "            count+=1\n",
    "            dummy_dict = paper_info(paper)\n",
    "            df_dict['title'].append(dummy_dict['title'])\n",
    "            df_dict['id'].append(dummy_dict['urlid'])\n",
    "            df_dict['abstract'].append(dummy_dict['abstract'])\n",
    "            df_dict['authors'].append(dummy_dict['authors'])\n",
    "            df_dict['journal'].append(dummy_dict['journal'])\n",
    "            df_dict['published'].append(dummy_dict['published'])\n",
    "        print('papers added on this scrape: ',count)\n",
    "        return df_dict\n",
    "\n",
    "    # Submitting request to scrape different pages\n",
    "    for obj in scraping_arxiv(total_papers=30000, max_results=2000, search_query=f'all:{query}'):\n",
    "        create_dict(obj, df_dict)\n",
    "        time.sleep(3)\n",
    "\n",
    "    # Creating a dataframe\n",
    "    df_arxiv = pd.DataFrame(df_dict)\n",
    "\n",
    "    # Dropping duplicates\n",
    "    df_arxiv.drop_duplicates(subset='title', inplace=True)\n",
    "\n",
    "    # Creating a csv file\n",
    "    df_arxiv.to_csv(f'{path_to_arxiv_data}arxiv_{query}_30000.csv', index = False)\n",
    "    \n",
    "\n",
    "for query in search_queries:\n",
    "    print(f\"Starting scraping arXiv for papers under '{query}' ...\")\n",
    "    one_big_scraping_function(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeeeeeeeHAaaaaaaa!\n"
     ]
    }
   ],
   "source": [
    "print(\"HeeeeeeeHAaaaaaaa!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
